# Gemma GPU Service Dockerfile
# GPU-enabled container for Cloud Run deployment with NVIDIA L4 GPU
# Region: europe-west1
# GPU: NVIDIA L4 (1 GPU)
# Memory: 16Gi
#
# Build optimization strategy:
# 1. Dependencies installed first (changes rarely) - better layer caching
# 2. Code copied last (changes frequently) - rebuild only code layer
# 3. Pin versions where possible for reproducible builds

# Use PyTorch base image with CUDA support
# This image includes PyTorch with CUDA 12.1 support
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime AS base

WORKDIR /app

# Install system dependencies (cached layer unless apt packages change)
RUN apt-get update && apt-get install -y \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies with pip cache for faster rebuilds
# Pin versions for better reproducibility and caching
# This layer is cached unless requirements change
RUN pip install --no-cache-dir \
    transformers==4.40.0 \
    accelerate==0.30.0 \
    bitsandbytes==0.41.0 \
    fastapi==0.115.0 \
    "uvicorn[standard]==0.30.0" \
    pydantic==2.0.0 \
    python-dotenv==1.0.0 \
    huggingface-hub==0.23.0

# Set environment variables before copying code
# These rarely change, so they can be cached
ENV PYTHONUNBUFFERED=1
ENV PORT=8080
ENV MODEL_NAME=google/gemma-7b-it

# Copy Gemma service code last (this changes most frequently)
# Only this layer rebuilds when code changes, dependencies remain cached
COPY gemma_service/ ./gemma_service/

# Expose port (Cloud Run sets PORT env var automatically)
EXPOSE 8080

# Health check (Cloud Run uses this)
HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:${PORT}/healthz || exit 1

# Run FastAPI app
# Cloud Run sets PORT env var automatically, read it dynamically
CMD python -c "import os; port = int(os.getenv('PORT', 8080)); import uvicorn; uvicorn.run('gemma_service.main:app', host='0.0.0.0', port=port)"

